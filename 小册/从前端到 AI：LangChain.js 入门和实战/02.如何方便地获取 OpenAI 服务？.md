# 如何方便地获取 OpenAI 服务？

我们这一节介绍如何最方便的获得 OpenAI 的服务，其中会介绍除 OpenAI 官方 API 外几种常见获取 llm 服务的方式，以及如何在 langchain 中使用。

如果你能获得稳定的官方 API，那可以跳过本章的大部分内容。

## Azure OpenAI

Azure OpenAI 的优势是跟 OpenAI 同源，并且国内付款比较容易。

正常注册 microsoft 账号，并注册登录 azure [link](https://azure.microsoft.com/en-us/)。这里注册 azure 的时候，需要手机号验证码，国内正常 +86 手机即可。还需要一张信用卡，在不开启付费业务的情况下不会有支出。

我为了这个教程新注册了一个 azure 账号，会送 200 刀的的额度帮助大家上手，这个额度是有期限的。具体大家注册时候的活动不确定，但看起来是个长期的活动。

## 第三方 OpenAI 服务

另一种，就是经过中转的第三方 OpenAI 服务，这类平台比较多，我们不做推荐，只讲解一下如何在 langchain 中使用。

首先是在 .env 声明 key

```js
OPENAI_API_KEY = abc;
```

然后在创建 ChatOpenAI 时，指定 baseUrl：

```js
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage } from "@langchain/core/messages";

const chatModel = new ChatOpenAI({
  configuration: {
    baseURL: "xxx",
  },
});

await chatModel.invoke([new HumanMessage("Tell me a joke")]);
```

## 本地大模型

如果你是 win 平台，显卡显存大于 6G，mac 平台 M 系芯片 + 16G 内存基本就足够运行 7B 大小的模型。虽然推理速度较慢，但可以应付一些本地的测试。

在 mac 平台下，我推荐用 [ollma](https://ollama.com/)，使用起来非常简单，下载好模型后，点开这个 app 后，就会自动在 http://localhost:11434 host 一个 llm 的服务。
如果是 win 平台，可以尝试一下 LM Studio，其提供的模型更多，可玩性也更强一些。

目前我本地使用的还是 llama2，最新的已经到了 llama3，大家可以在这 [github](https://github.com/ollama/ollama) 找到目前支持的模型，llama 和 Mistral 家族的模型效果都很棒。

然后，我们就可以在 langchian 中使用这些本地模型：

```js
import { Ollama } from "@langchain/community/llms/ollama";

const ollama = new Ollama({
  baseUrl: "http://localhost:11434",
  model: "llama2",
});

const res = await ollama.invoke("讲个笑话");
```

如果你使用的是 deno，需要在 deno.json 中加入这一行依赖别名：

```json
{
    "imports":{
        ...
        "@langchain/community/": "npm:/@langchain/community/",
        ...

    }
}

```

大家可以直接用 ollama 来代替小册中出现的 llm 模型，当然其效果肯定不如 gpt3.5 和 gpt4 强。但如果你不容易获得 openAI 的 API，使用本地模型进行学习和测试，也是一个省钱和方便的方案。

## 加载环境变量

首先是在 nodejs 中，我们使用 dotenv/config 这个第三方库：

```sh
yarn add dotenv/config
```

然后，在需要使用环境变量的 js 文件中：

```js
import "dotenv/config";
```

即可，.env 中的环境变量就会被注入到 process.env 中。

在 Deno 中稍有不同，因为 langchain 是为 nodejs 设计，所以读取环境变量时会默认从 process.env 中进行读取，所以我一般会这样 hack 一下：

```js
import { load } from "https://deno.land/std@0.223.0/dotenv/mod.ts";
const env = await load();

const process = {
  env,
};
```

即，从 .env 文件加载出来所有的环境变量后，再自己创建一个全局的 process.env 方便 langchain 进行读取。
